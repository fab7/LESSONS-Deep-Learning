{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation exercise\n",
    "\n",
    "Now you're going to implement the backprop algorithm for a network trained on the graduate school admission data. You should have everything you need from the previous exercises to complete this one.\n",
    "\n",
    "Your goals here:\n",
    "\n",
    " - Implement the forward pass.\n",
    " - Implement the backpropagation algorithm.\n",
    " - Update the weights.\n",
    "\n",
    "To experiment the code and print out the MSE, as well as the accuracy on a test set, the fraction of correctly predicted admissions, use the \"Run\" button from the top bar. After you are ready for grading, run the training by pressing \"TEST CODE\" to submit your code. Make sure to remove all cells you added before submitting your work.\n",
    "\n",
    "Note: This code takes a while to execute, so Udacity's servers sometimes return with an error saying it took too long. If that happens, it usually works if you try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.2763000206585236\n",
      "Train loss:  0.27487280940102565\n",
      "Train loss:  0.27348146900538234\n",
      "Train loss:  0.2721253511981268\n",
      "Train loss:  0.2708037972995826\n",
      "Train loss:  0.2695161402601932\n",
      "Train loss:  0.2682617065761968\n",
      "Train loss:  0.26703981808591787\n",
      "Train loss:  0.26584979364857986\n",
      "Train loss:  0.26469095070807314\n",
      "Prediction accuracy: 0.425\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from data_prep import features, targets, features_test, targets_test\n",
    "np.random.seed(21)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden  = 2  # number of hidden units\n",
    "epochs    = 900\n",
    "learnrate = 0.005\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights\n",
    "weights_input_hidden  = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5,\n",
    "                                         size=n_hidden)\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ##################\n",
    "        ## Forward pass ##\n",
    "        ##################\n",
    "        # 1f) Calculate the output\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "        output = sigmoid(np.dot(hidden_output,\n",
    "                                weights_hidden_output))\n",
    "        ###################\n",
    "        ## Backward pass ##\n",
    "        ###################\n",
    "        # 1b) Calculate the network's prediction error\n",
    "        error = y - output\n",
    "        \n",
    "        # 2b) Calculate error term for the output unit\n",
    "        output_error_term = error  *output*  (1 - output)\n",
    "\n",
    "        ## Propagate errors to hidden layer\n",
    "        \n",
    "        # 3b) Calculate the hidden layer's contribution to the error\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "        \n",
    "        # 4b) Calculate the error term for the hidden layer\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "        \n",
    "        # 5b) Update the change in weights\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "    # Update weights\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output,\n",
    "                             weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(\"Prediction accuracy: {:.3f}\".format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
